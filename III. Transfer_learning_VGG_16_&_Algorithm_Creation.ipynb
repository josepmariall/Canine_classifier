{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "III. Transfer learning : VGG-16 & Algorithm Creation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "J5oKytTirOMn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# III. Transfer Learning : VGG-16 & Algorithm Creation\n",
        "\n",
        "In this notebook, we shall try to create a model using top notch Convolutional Neural Networks for image Classification : \n",
        "  \n",
        "  * VGG16 \n",
        "\n",
        "On the previous jupyter file (II. Transfer learning : DENSENET 161) we obtained an accuracy of 82%, way higher than on the first model created from sctatch (10%) and with only a very few epochs.\n",
        "\n",
        "We'll see what happens to accuracy when testing VGG-16\n",
        "\n",
        "Since this model produces the best results, we shall use it to create an algorithm to classify humans and dogs in the next jupyter notebook (IV)."
      ]
    },
    {
      "metadata": {
        "id": "IE-4cLG2V1pa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 0.Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "ADnq3OuGsOPp",
        "colab_type": "code",
        "outputId": "2dbb12da-67c9-4ef5-97e5-c6d1e766357a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KlYKjHOjl534",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We start by importing libraries\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-VMyUSeedEK",
        "colab_type": "code",
        "outputId": "09e4941d-c3be-47c4-f9f5-ccb8486c7477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5SScQPdFfyY7",
        "colab_type": "code",
        "outputId": "961fd50f-b31f-40e0-ad7e-23145b85a255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m10HG5BUekAS",
        "colab_type": "code",
        "outputId": "100dbfb0-0c46-4a26-b872-9873f2ca8365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aNXp5RZMvYfs",
        "colab_type": "code",
        "outputId": "6d9f7673-322e-4514-9103-e1079519877f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading the dog and human dataset\n",
        "from IPython.display import clear_output\n",
        "!wget -cq -O dogImages.zip https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
        "!wget -cq -O lfw.tgz http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "clear_output()  \n",
        "print(\"Downloaded Successfully\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "57abcI6LvY0P",
        "colab_type": "code",
        "outputId": "d2203ea1-9c16-4ce2-e022-4be239d9cb37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Extractting the datasets\n",
        "!unzip -n dogImages.zip\n",
        "!tar -xvzf lfw.tgz\n",
        "clear_output()\n",
        "print(\"Extracted Successfully\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dMj5bHPZvZDg",
        "colab_type": "code",
        "outputId": "bf4930db-ec81-4c88-9769-3d044c524db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading the pre-trained face detector\n",
        "import os\n",
        "if not os.path.isdir('haarcascades'):\n",
        "    os.mkdir('haarcascades')\n",
        "!wget -cq -O haarcascades/haarcascade_frontalface_alt.xml https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/project-dog-classification/haarcascades/haarcascade_frontalface_alt.xml\n",
        "clear_output()\n",
        "print(\"Downloaded Successfully\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D3rqLZTGCwYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.Prepare images for classification"
      ]
    },
    {
      "metadata": {
        "id": "og96seFoPzgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#The following code will help us when dealing with images to be used in different classification models#\n",
        "#from PIL import Image\n",
        "#import torchvision.transforms as transforms\n",
        "#from torch.autograd import Variable\n",
        "\n",
        "def process_image_to_tensor(image):\n",
        "\n",
        "    # define transforms for the training data and testing data\n",
        "    prediction_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                                          transforms.CenterCrop(224),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                               [0.229, 0.224, 0.225])])\n",
        "    \n",
        "    img_pil = Image.open( image ).convert('RGB')\n",
        "    img_tensor = prediction_transforms( img_pil )[:3,:,:].unsqueeze(0)\n",
        "    \n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "# helper function for un-normalizing an image  - from STYLE TRANSFER exercise\n",
        "# and converting it from a Tensor image to a NumPy image for display\n",
        "def image_convert(tensor):\n",
        "    \"\"\" This is to display a tensor as an image. \"\"\"\n",
        "    \n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ofFaNniZQktS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Specify Directories"
      ]
    },
    {
      "metadata": {
        "id": "IjMyAVisQhiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specify directories\n",
        "data_dir = 'dogImages'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "test_dir = data_dir + '/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmwqK40fQcjk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "### TODO: Write data loaders for training, validation, and test sets\n",
        "## Specify appropriate transforms, and batch_sizes\n",
        "# Batch size\n",
        "batch_size = 20\n",
        "# For faster computation, setting num_workers\n",
        "num_workers = 0\n",
        "\n",
        "# Transforms for the training, validation, and testing sets\n",
        "data_transforms = {\n",
        "    'train'      : transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])]),\n",
        "\n",
        "    'valid'      : transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])]),\n",
        "    'test'       : transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "}\n",
        "\n",
        "# Loading the datasets with ImageFolder\n",
        "image_datasets = {\n",
        "    'train'  : datasets.ImageFolder(train_dir, transform=data_transforms['train']),\n",
        "    'valid'  : datasets.ImageFolder(valid_dir, transform=data_transforms['valid']),\n",
        "    'test'   : datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
        "}\n",
        "\n",
        "# Using the image datasets and the trainforms to define dataloaders\n",
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(image_datasets['train'], batch_size = 32, shuffle=True, num_workers = num_workers),\n",
        "    'valid' : torch.utils.data.DataLoader(image_datasets['valid'], batch_size = 16),\n",
        "    'test'  : torch.utils.data.DataLoader(image_datasets['test'], batch_size = 16)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74-NSjtmQSDR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "QVBFHYo_-NNj",
        "colab_type": "code",
        "outputId": "fd2876bd-9169-4178-8a5b-44f571503dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#Find out the number of images for each category\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# load filenames for human and dog images\n",
        "human_files = np.array(glob(\"lfw/*/*\"))\n",
        "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
        "\n",
        "# print number of images in each dataset\n",
        "print('There are %d total human images.' % len(human_files))\n",
        "print('There are %d total dog images.' % len(dog_files))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 13233 total human images.\n",
            "There are 8351 total dog images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kTea65FMOr5g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.Importing VGG-16"
      ]
    },
    {
      "metadata": {
        "id": "s3VLUbtjOrtt",
        "colab_type": "code",
        "outputId": "c4d0ee25-f3f6-44b8-8761-acb1808f24a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Is GPU available: ', 'Yes' if torch.cuda.is_available() else 'No')\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is GPU available:  Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cZYwlIcEPExP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Now I try to do the same but with VGG16 instead\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Loading the pretrained model\n",
        "model_transfer2 = models.vgg16(pretrained=True)\n",
        "\n",
        "if use_cuda:\n",
        "    model_transfer2 = model_transfer2.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pVDFEJ0cPfCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1 VGG-16 model architecture\n",
        "\n",
        "In this part I shall modify the model architecture, I want to use the first layers as feature extractor and then replace the last one for classifying my model. "
      ]
    },
    {
      "metadata": {
        "id": "6JDJ2KKhPePQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# freeze parameters so we don't backprop through them\n",
        "for param in model_transfer2.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace the last fully connected layer with a Linnear layer with 133 out features (param_output_size)\n",
        "num_features = model_transfer2.classifier[6].in_features\n",
        "features = list(model_transfer2.classifier.children())[:-1]\n",
        "features.extend([nn.Linear(num_features, 133)])\n",
        "model_transfer2.classifier = nn.Sequential(*features)\n",
        "\n",
        "if use_cuda:\n",
        "    model_transfer2 = model_transfer2.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAw3l5NuTRHx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Specify Loss Function and Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "v1hhS1aMTPET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Selecting the loss function and optimizer\n",
        "criterion_transfer2 = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_transfer2 = optim.SGD(model_transfer2.classifier.parameters(), lr=0.01)\n",
        "\n",
        "# Moving the model to the device\n",
        "model_transfer2.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nW2KPUfZTb8Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Train and Validate the model"
      ]
    },
    {
      "metadata": {
        "id": "zwUhClXhXtHP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part helps building a robust training to deal with truncated images\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    print(\"start training for {} epochs ...\".format(n_epochs))\n",
        "\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    \n",
        "    # exist save-file, load save file\n",
        "    if os.path.exists(save_path):\n",
        "        print(\"load previous saved model ...\")\n",
        "        model.load_state_dict(torch.load(save_path))\n",
        "    \n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            \n",
        "            ## record the average training loss, using something like\n",
        "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            \n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## update the average validation loss\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # update average validation loss \n",
        "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "        \n",
        "     \n",
        "        # print training/validation statistics \n",
        "        print('\\n-----------------------------------------------------------------------------\\nEpoch: {} \\nTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "        \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss has decreased from ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "            \n",
        "    # return trained model\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-h20STheTa3N",
        "colab_type": "code",
        "outputId": "63fac082-e8ea-4ebb-acbe-050fc1b5f1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "loaders_transfer2 = loaders\n",
        "\n",
        "# train the model\n",
        "model_transfer2 = train(2, loaders_transfer2, model_transfer2, optimizer_transfer2, criterion_transfer2, use_cuda, 'model_transfer2.pt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training for 2 epochs ...\n",
            "load previous saved model ...\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Epoch: 1 \n",
            "Training Loss: 0.958612 \tValidation Loss: 0.440452\n",
            "Validation loss has decreased from (inf --> 0.440452).  Saving model ...\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Epoch: 2 \n",
            "Training Loss: 0.955146 \tValidation Loss: 0.394805\n",
            "Validation loss has decreased from (0.440452 --> 0.394805).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lOZm71EZdLFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model has been trained for 15 epochs through different rounds"
      ]
    },
    {
      "metadata": {
        "id": "_o3PFFJRTnGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Test the model "
      ]
    },
    {
      "metadata": {
        "id": "eOJhyb5vTlQv",
        "colab_type": "code",
        "outputId": "13ea93b3-8ce6-4614-a8bd-f6c706604c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "def test(loaders, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# call test function    \n",
        "test(loaders, model_transfer2, criterion_transfer2, use_cuda)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.467801\n",
            "\n",
            "\n",
            "Test Accuracy: 85% (715/836)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u2w9I-e4t5hD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5 Save the model\n"
      ]
    },
    {
      "metadata": {
        "id": "sK6zixRHt4Fk",
        "colab_type": "code",
        "outputId": "1de5ee34-be47-4ae6-f33a-9a04d98a5f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Our model: \\n\\n\", model_transfer2, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model_transfer2.state_dict().keys())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model: \n",
            "\n",
            " VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Dropout(p=0.5)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.5)\n",
            "    (6): Linear(in_features=4096, out_features=133, bias=True)\n",
            "  )\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['features.0.weight', 'features.0.bias', 'features.2.weight', 'features.2.bias', 'features.5.weight', 'features.5.bias', 'features.7.weight', 'features.7.bias', 'features.10.weight', 'features.10.bias', 'features.12.weight', 'features.12.bias', 'features.14.weight', 'features.14.bias', 'features.17.weight', 'features.17.bias', 'features.19.weight', 'features.19.bias', 'features.21.weight', 'features.21.bias', 'features.24.weight', 'features.24.bias', 'features.26.weight', 'features.26.bias', 'features.28.weight', 'features.28.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.6.weight', 'classifier.6.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CwYCwdCwFu9C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    \"classifier\": model_transfer2.classifier,\n",
        "    \"state_dict\": model_transfer2.state_dict()\n",
        "}\n",
        "torch.save(checkpoint, 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CHGPUtpogJ7_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.Evaluation "
      ]
    },
    {
      "metadata": {
        "id": "duxTBE_xechQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see now accuracy of the classifier using VGG -16 goes up to 85% in only 11 epochs.  \n",
        "Let's test on another document another top notch classifier, VGG-16\n",
        "\n",
        "Certainly, I could improve the model by further adding elements to data agumentation, and by increasing the number of epochs. \n",
        "\n",
        "Comparing DENSENET 161 and VGG - 16, clearly we can see VGG-16 produces better results. \n",
        "\n",
        "So I will take VGG - 16 to write an algoirthm to detect humans, dogs or niether. This will be done in a new jupyter notebook. \n"
      ]
    },
    {
      "metadata": {
        "id": "9LlcMbjW9uyr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}
